{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "creator": "admin",
    "createdOn": 1645634285332,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 38,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import datetime\nlast_day_last_month\u003ddatetime.date.today().replace(day\u003d1)- datetime.timedelta(days\u003d1)\nlast_end_date\u003dlast_day_last_month.strftime(\"%Y-%m-%d\")\nlast_end_date"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "\u00272022-05-31\u0027"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 44,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "datetime.date(2022,6,30)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "datetime.date(2022, 6, 30)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 47,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "datetime.date(2022, 5, 31)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport datetime\n\n# get project\nprojecthandle\u003ddataiku.api_client().get_project(dataiku.default_project_key())\n\n# get variables\nvars\u003dprojecthandle.get_variables()\n\n# compute accurate value\ntoday\u003ddatetime.date.today()\ntoday_month\u003dtoday.month\n\ntomorrow\u003ddatetime.date.today()+datetime.timedelta(days\u003d1)\ntomorrow_month\u003dtomorrow.month\n\nif today_month\u003d\u003dtomorrow_month:#the month is not over\n    last_day_last_month\u003ddatetime.date.today().replace(day\u003d1)- datetime.timedelta(days\u003d1)\n    last_end_date\u003dlast_day_last_month\nelse: # the month is over\n    last_end_date\u003ddatetime.date.today()\n\nlast_end_date\u003dlast_end_date.strftime(\"%Y-%m-%d\")\n\n# update variable value\nvars[\"standard\"][\"last_end_date\"]\u003dlast_end_date\n\n# update project variables\nprojecthandle.set_variables(vars)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "last_day_of_prev_month \u003d date.today().replace(day\u003d1) - timedelta(days\u003d1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# PROJECTS"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "hidden": true
      },
      "source": [
        "# create a client\nclient\u003ddataiku.api_client()\nprojects\u003dclient.list_projects()\n\n# print all projects\nfor p in projects:\n    print (p[\u0027projectKey\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# print current project\ndataiku.default_project_key()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# DATASETS"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"mydataset\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "client\u003ddataiku.api_client()\nif dataiku.default_project_key()\u003d\u003d\u0027ENERGYCONSUMPTIONPREDICTION\u0027:\n    outcome\u003d\u0027SUCCESS\u0027\nelse: \n    outcome\u003d\u0027FAILED\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "dataset\u003ddataiku.Dataset(\"full_data_enriched\")\nfull_data_df"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "Read the schema"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "df.read_schema()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name \u0027df\u0027 is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-1-b5acd4be053b\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name \u0027df\u0027 is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# FOLDERS"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "hidden": true
      },
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Recipe inputs\ninput_dataset_name \u003d dataiku.Dataset(\"input_dataset_name\")\ninput_dataset_name_df \u003d input_dataset_name.get_dataframe()\n\n# Recipe outputs\nfolder_name \u003d dataiku.Folder(\"folder_id\")\nfolder_name_path \u003d folder_name.get_path()\n\nimport time\ncurrent_day \u003d time.strftime(\"%Y-%m-%d\")\ninput_dataset_name_df.to_csv(path_or_buf\u003dfolder_name_path+\"export_\"+current_day+\".csv\", index\u003dFalse)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "## Write a dataset in a folder"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# Read recipe inputs and get a dataframe\ndataset \u003d dataiku.Dataset(\"data_by_state_filtered_ordered\")\ndf \u003d dataset.get_dataframe()\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# folder handle\nfolder_handle \u003d dataiku.Folder(\"s6qy0GK5\")\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# location wished in the output folder\npath_output_folder\u003d\"datasets\"\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nfinal_path\u003dos.path.join(path_output_folder,\"my_file.csv\")\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nwith folder_handle.get_writer(final_path) as writer:\n    writer.write(df.to_csv().encode(\"utf-8\"))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "## Load a pkl model stored in a folder. "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "from sklearn.externals import joblib\nimport os\n\n# input folder\nfolder_handle \u003d dataiku.Folder(\"1UUJ7dDF\")\n\n# get its path\nfolder_info \u003d folder_handle.get_info()\n\n# load the model as pkl file\nclf \u003d joblib.load(os.path.join(folder_info[\u0027path\u0027], \u0027model.pkl\u0027))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "## Copy paste some files in the same folder but with a new name"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# import packages\nimport dataiku\n\n# get the project handle \nprojecthandle\u003ddataiku.api_client().get_project(dataiku.default_project_key())\n\n# get the current variable (country for you, year for me)\nmy_vars\u003dprojecthandle.get_variables()\nyear\u003dmy_vars[\"standard\"][\"selected_year\"]\n\n# get the final folder (you will find the folder ID in the URL that appears when you\u0027re in the folder)\nfolder_handle\u003ddataiku.Folder(\"xtJHWex9\")\n\n# path of your original dashboard (basically the name of the file preceded by a \"/\")\npath\u003d\"/Year-level-dashboard.pdf\"\n\n# new path for the new file (here I put the year (country for you) + \n# the current timestamp but it\u0027s up to you how you name it)\nfrom datetime import datetime\nnow\u003ddatetime.now()\nformatted_now\u003dstr(now.year)+\"-\"+str(now.month)+\"-\"+str(now.day)\nnew_path\u003d\"/year_\"+str(year)+\"_dashboard_exported_on_\"+formatted_now+\".pdf\"\n\n# copy the original file with a new file name that corresponds to country and date\n\nwith folder_handle.get_download_stream(path) as f:\n    data\u003df.read()\n\nwith folder_handle.get_writer(new_path) as w:\n    w.write(data)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# VARIABLES"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nimport datetime\n\n# get project\nprojecthandle\u003ddataiku.api_client().get_project(dataiku.default_project_key())\n\n# get variables\nvars\u003dprojecthandle.get_variables()\n\n# compute accurate value\nnow\u003ddatetime.datetime.now()\nten_days_before\u003dnow-datetime.timedelta(days\u003d10)\nlast_end_date\u003dten_days_before.strftime(\"%Y-%m-%d\")\n\n# update variable value\nvars[\"standard\"][\"last_end_date\"]\u003dlast_end_date\n\n# update project variables\nprojecthandle.set_variables(vars)"
      ],
      "outputs": []
    },
    {
      "execution_count": 31,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nprojecthandle.get_variables()[\u0027standard\u0027][\"nested_var\"][\"var_1\"]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "\u0027test\u0027"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 32,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "dataiku.get_custom_variables()[\"nested_var.var_1\"] "
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\u0027nested_var.var_1\u0027",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-32-5c8843bd6327\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_custom_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nested_var.var_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: \u0027nested_var.var_1\u0027"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# PARTITIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "### On peut spécifier une recette générale puis expliciter la partition que l\u0027on veut construire avec l\u0027engrenage! "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# lister les partitions\ndataset.list_partitions()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "hidden": true
      },
      "source": [
        "partitioned_code_input_df \u003d input_df # For this sample code, simply copy input to output\n\n# cette cellule ne marchera pas dans un notebook (mais ok dans une recette)\npartitioned_code_input_df[\"ma_partition\"] \u003d dataiku.dku_flow_variables[\"DKU_DST_DATE\"]\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "Read only certain partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "\u003ccode\u003einput_ds \u003d dataiku.Dataset(\"input\")\ninput_ds.read_partitions \u003d [\u00272013-09-19\u0027, \u00272013-09-26\u0027]\ndf_part \u003d input_ds.get_dataframe()\n\u003c/code\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# CUSTOM METRICS AND CHECKS"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# example for a metric in sql\n\nselect count(distinct \"merchant_id\") as unique_merchants \nfrom ${DKU_DATASET_TABLE_NAME}\n\n\n# example for a metric in python\ndef process(dataset, partition_id):\n    df \u003d dataset.get_dataframe()\n    max_purchase_amt \u003d df[\"purchase_amount\"].max()\n    max_purchase_amt_euro \u003d 0.85 * max_purchase_amt\n    return {\"max_purchase_in_euro\": max_purchase_amt_euro}\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# FORMULA LANGUAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "\u003ccode\u003e\n1\tJan\n2\tFeb\n3\tMar\n4\tApr\n5\tMay\n6\tJun\n7\tJul\n8\tAug\n9\tSep\n10\tOct\n11\tNov\n12\tDec \u003c/code\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# SCENARIOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "Prototype a scenario"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\n\n# define a list\nyear_list\u003d[1,2]\n\n# get project handle\nprojecthandle\u003ddataiku.api_client().get_project(dataiku.default_project_key())\n\nfor year in year_list:\n    \n    # UPDATE THE VARIABLE\n\n    # get variables\n    my_vars\u003dprojecthandle.get_variables()\n    \n    \n    # update variable value\n    my_vars[\"standard\"][\"selected_year\"]\u003dyear\n\n    # update project variables\n    projecthandle.set_variables(my_vars)\n    \n    \n    # RUN THE SCENARIO\n    scenario \u003d projecthandle.get_scenario(\"Build_a_state_dashboard_and_send_it\")\n    scenario.run_and_wait()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FLOW"
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\n\nclient \u003d dataiku.api_client()\n# enter in your project ID \nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "flow \u003d project.get_flow()\n# gets the graph for a given project flow \ngraph \u003d flow.get_graph()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 10,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#direction should be either \u0027successors\u0027 for downstream or \u0027predecessors\u0027 for upstream \n\ndef get_nodes(node, node_array, direction\u003d\u0027successors\u0027):\n    nodeval \u003d graph.nodes[node]\n    if nodeval:\n        if nodeval[\u0027type\u0027] \u003d\u003d \u0027COMPUTABLE_DATASET\u0027:\n            node_array.append(nodeval[\u0027ref\u0027])\n        for connected_node in nodeval[direction]:\n            get_nodes(connected_node, node_array, direction)\n    return node_array  "
      ],
      "outputs": []
    },
    {
      "execution_count": 14,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "downstream\u003d[]\ndownstream\u003dget_nodes(\u0027full_data_prepared\u0027,downstream,direction\u003d\u0027successors\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 15,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "downstream"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "[\u0027full_data_prepared\u0027,\n \u0027full_data_prepared_filtered\u0027,\n \u0027full_data_prepared_filtered_by_State_Factor\u0027,\n \u0027data_by_state_filtered_ordered\u0027,\n \u0027full_data_enriched\u0027,\n \u0027full_data_enriched_windows\u0027,\n \u0027full_data_enriched_windows_prepared\u0027,\n \u0027full_data_second_window\u0027,\n \u0027to_predict\u0027,\n \u0027to_predict_scored\u0027,\n \u0027to_train\u0027,\n \u0027train_set\u0027,\n \u0027to_predict_scored\u0027,\n \u0027validation_set_scored\u0027,\n \u0027metrics\u0027,\n \u0027validation_set\u0027,\n \u0027validation_set_scored\u0027,\n \u0027metrics\u0027,\n \u0027classif_train\u0027,\n \u0027classif_train_prepared\u0027,\n \u0027training\u0027,\n \u0027validation\u0027,\n \u0027classif_eval\u0027,\n \u0027full_data_with_code\u0027,\n \u0027full_data_infered_pandas\u0027]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 22,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for name in downstream:\n    dataset\u003dproject.get_dataset(name)\n    job\u003ddataset.build()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "DataikuException",
          "evalue": "Job run did not finish. Status: FAILED",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataikuException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-22-56f786cc3a23\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdownstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m     \u001b[0mjob\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/Users/carolineboudier/Desktop/dataiku-dss-10.0.0-osx/python/dataikuapi/dss/dataset.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, job_type, partitions, wait, no_fail)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mjd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 290\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_and_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mjd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Users/carolineboudier/Desktop/dataiku-dss-10.0.0-osx/python/dataikuapi/dss/project.py\u001b[0m in \u001b[0;36mstart_and_wait\u001b[0;34m(self, no_fail)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mjob\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mwaiter\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mDSSJobWaiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1842\u001b[0;31m         \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_fail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Users/carolineboudier/Desktop/dataiku-dss-10.0.0-osx/python/dataikuapi/dss/job.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, no_fail)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 65\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job run did not finish. Status: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjob_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataikuException\u001b[0m: Job run did not finish. Status: FAILED"
          ]
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}